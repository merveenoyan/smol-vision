{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/merveenoyan/smol-vision/blob/main/Faster_Zero_shot_Object_Detection_with_Optimum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQGFrJu77ZBZ"
   },
   "source": [
    "## Faster and Smaller Zero-shot Object Detection with Optimum\n",
    "\n",
    "In this guide, we will see how to quantize and perform graph optimizations over the powerful OWL model using Optimum.\n",
    "\n",
    "Let's start by installing `Optimum` and `ONNXRuntime` for GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for CUDA 11.8 use https://download.pytorch.org/whl/cu118\n",
    "# for CUDA 12.1 use https://download.pytorch.org/whl/cu121\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PE_q6kFrqraF",
    "outputId": "819677d2-38b6-457d-9028-6685210efbc7"
   },
   "outputs": [],
   "source": [
    "%pip install optimum onnxruntime-gpu onnx\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpYGRFK55Kj_"
   },
   "source": [
    "We will run `notebook_login()` to login to Hugging Face Hub. This way, we can push and pull our ONNX model and quantized/optimized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "0d77c99c637c4251b0275aefb3907a0c",
      "bfd82f1fa857446ebc11f3ee85998fa8",
      "9c88822c78344c3d8f66efa376efb67f",
      "911ce3a4a9c14b6788f0ee02209d3dc0",
      "95b1977fa930471cb92d3db04c4f744c",
      "60e34263a07c469ca14c9f33cb2afa16",
      "d36c575e9a574ef58dd82f3362cac133",
      "69dd787d1a414dd99e4dc10f6346bd97",
      "65a60d22a8a84514b4520613478849b7",
      "1058ecbb025e4dfabbe9d7ca7b98198c",
      "00ecabd734f441e7a03f400c894c6f9e",
      "e794617411ef45dba87f156351eb8636",
      "2e59bbc337ee4ab1a3a1722e8eca9cc6",
      "2371b130a879430ca87773b173e83cb4",
      "6a9d5006881346b6a945a30a2b8545b5",
      "ce5c33b6af2240818428348004f37aeb",
      "5374148f267f4989b98e4aab3bd79262",
      "808b0b7be987408790df416f65a2204a",
      "1b0e4a7cd0ae41fe855d63697f6c7b19",
      "45e9d716b9244ee6883850adf9daa260",
      "af218beafa794a28b0d67ce1f838edbd",
      "4a7955336a754e2d8944a5bebd1a8927",
      "1a032a5285674da8aa9533e9970e05e7",
      "1a27acd4745a48c080fc007df491956a",
      "a2a3613f8ba346dc9a905d6153995d64",
      "3b0fc2dd7770407f987ceb7a4d536b94",
      "396e335b974945e7bbc758729e902034",
      "eeda05c40f5f46d8909c79766977b3b7",
      "aa64ac2d50704dc7afb903bc769fead7",
      "afc317e6d50f47868e9d1530962a8e32",
      "fdd6ae08102e41a9aeab153b8563b344",
      "4000ef0fbfc140ac848e52d9f781e9ea"
     ]
    },
    "id": "BWO2TLTa5BvZ",
    "outputId": "0629fac0-79ae-416a-8406-e77f4ae8c9c5"
   },
   "outputs": [],
   "source": [
    "#from huggingface_hub import notebook_login\n",
    "#notebook_login()\n",
    "\n",
    "import os\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "\n",
    "hf_token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "api = HfApi(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuMpsYOQjaJ7"
   },
   "source": [
    "## Load the Model and Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zV8sUJXE6mV3"
   },
   "source": [
    "We will use the [fine-tuned OWLv2](https://huggingface.co/google/owlv2-large-patch14-ensemble) model. This model is known to perform better on fine-tuned classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oObnJ8Pg_7r5"
   },
   "source": [
    "Let's benchmark the PyTorch model first in different precisions and see how the model performs in them. Then we will export the model, quantize and compare the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bs9FTTS8CnEw"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoProcessor, Owlv2ForObjectDetection\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "texts = [[\"cat\", \"dog\"]]\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTM0IceKXiW8"
   },
   "source": [
    "Get to benchmark. Note that I'm doing CPU inference here for those who don't have an A100 to test this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wsM1ych2DJtG",
    "outputId": "b05808d9-abb9-45ab-ed97-4ea1979da8c2"
   },
   "outputs": [],
   "source": [
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "repetitions = 30\n",
    "timings=np.zeros((repetitions,1))\n",
    "\n",
    "# warmup\n",
    "for _ in range(10):\n",
    "    _ = model(**inputs)\n",
    "\n",
    "# measure\n",
    "with torch.no_grad():\n",
    "    for rep in range(repetitions):\n",
    "      torch.cuda.synchronize()\n",
    "      starter.record()\n",
    "      _ = model(**inputs)\n",
    "      ender.record()\n",
    "      torch.cuda.synchronize()\n",
    "      curr_time = starter.elapsed_time(ender)\n",
    "      timings[rep] = curr_time\n",
    "\n",
    "mean_syn = np.sum(timings) / repetitions\n",
    "print(mean_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPzjjEjBS0Sd"
   },
   "source": [
    "Now let's see an actual model output and how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHCLAX6oKiU4",
    "outputId": "36e0ea7d-a6c7-4e17-c278-28763b752788"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)\n",
    "\n",
    "target_sizes = torch.Tensor([image.size[::-1]])\n",
    "results = processor.post_process_object_detection(\n",
    "    outputs=outputs, threshold=0.2, target_sizes=target_sizes\n",
    ")\n",
    "\n",
    "i = 0\n",
    "text = texts[i]\n",
    "boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "\n",
    "for box, score, label in zip(boxes, scores, labels):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "uqCRhuRWS_Ji",
    "outputId": "6d94a1c7-c57e-4d71-cc9f-57e5e836e903"
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image)\n",
    "\n",
    "for box, score, label in zip(boxes, scores, labels):\n",
    "  box = [round(i) for i in box.tolist()]\n",
    "  rectangle = patches.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], linewidth=2, edgecolor='b', facecolor='none')\n",
    "  ax.add_patch(rectangle)\n",
    "\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMBr0hrHVdPy"
   },
   "outputs": [],
   "source": [
    "# free up some space\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGadTgvT583r"
   },
   "source": [
    "## Export to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NHewD3D_hbu"
   },
   "source": [
    "We will export the loaded model and the processor to ONNX format and save them under another directory. Make sure to define the batch size as exported model expects the data to always be in certain shape.\n",
    "\n",
    "\n",
    "**Note**: When exporting to ONNX, the tracing warnings we'll for the assertions are not important. If you're exporting any other model and see a tracing error with a variable assignment feel free to open an issue in transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kVDGi6IpsxoM",
    "outputId": "30961d6a-9999-4f6f-e4ac-8b2fcdde5e7a"
   },
   "outputs": [],
   "source": [
    "!optimum-cli export onnx --model google/owlv2-base-patch16-ensemble --task zero-shot-object-detection --batch_size 1 owlv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ik-7290Yjj0W"
   },
   "source": [
    "## Quantize\n",
    "\n",
    "We can now quantize the model using Optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vdxe8UUEZBFe",
    "outputId": "32eff0d0-dbdc-4ef0-8e12-3ee40d48db3a"
   },
   "outputs": [],
   "source": [
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig, ORTConfig\n",
    "from optimum.onnxruntime.quantization import ORTQuantizer\n",
    "\n",
    "save_dir = \"quantized_model/\"\n",
    "\n",
    "quantizer = ORTQuantizer.from_pretrained(\"owlv2/\", file_name=\"model.onnx\")\n",
    "\n",
    "qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False)\n",
    "qconfig.operators_to_quantize=['MatMul', 'Attention']\n",
    "quantizer.quantize(save_dir=save_dir, quantization_config=qconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kf_ecvmmjoCs"
   },
   "source": [
    "## Benchmark Quantized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbAVMFkgWVqr"
   },
   "source": [
    "Time for a benchmark. First let's see file sizes of quantized model vs original model. There's a significant reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(f\"Listing file sizes of quantized model vs original model:\")\n",
    "\n",
    "# !ls -h -l /content/quantized_model/model_quantized.onnx\n",
    "file_name = \"quantized_model/model_quantized.onnx\"\n",
    "file_size = os.path.getsize(file_name) / 1024 / 1024\n",
    "print(f\"{file_name:<40}: {file_size:.2f} MB\")\n",
    "\n",
    "# !ls -h -l /content/owlv2/model.onnx\n",
    "file_name = \"owlv2/model.onnx\"\n",
    "file_size = os.path.getsize(file_name) / 1024 / 1024\n",
    "print(f\"{file_name:<40}: {file_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dTInp6bip4z"
   },
   "source": [
    "We will now check how the model performs. **Note that some degradation in IoU is expected since we reduced the model to half and we are pushing model outputs to be conservative with taking the highest score.**\n",
    "\n",
    "One should optimize the confidence filtering when working with quantized models (one could just stack the first three confident results and take the widest boundaries for instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzA2b6aA9OzO"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "from PIL import Image\n",
    "import onnxruntime as ort\n",
    "\n",
    "checkpoint = \"google/owlv2-base-patch16-ensemble\"\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)\n",
    "\n",
    "image = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n",
    "text_queries = [\"cat\", \"dog\"]\n",
    "\n",
    "np_inputs = processor(text=text_queries, images=image, return_tensors=\"np\")\n",
    "# Convert all int32 tensors to int64 in the inputs dictionary\n",
    "np_inputs = {key: value.astype(np.int64) if value.dtype == np.int32 else value for key, value in np_inputs.items()}\n",
    "# Create ONNX runtime session\n",
    "session = ort.InferenceSession(\"quantized_model/model_quantized.onnx\", providers=['CUDAExecutionProvider'])\n",
    "\n",
    "np_inputs = dict(np_inputs)\n",
    "# Run inference\n",
    "out = session.run(['logits', 'pred_boxes', 'text_embeds', 'image_embeds'], np_inputs)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fNIoyAJgSNr"
   },
   "source": [
    "out is a list of 4 elements, decoupled into two elements where first one is logits and second one is box predictions for each predicted element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAtp0vJ6f7Cg"
   },
   "source": [
    "We need to create a dummy object to handle the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBFGITR-f60g"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class OwlOutput:\n",
    "    logits: torch.Tensor\n",
    "    pred_boxes: torch.Tensor\n",
    "\n",
    "owl_out_1 = OwlOutput(logits = torch.from_numpy(out[0]).to(\"cuda\"),\n",
    "                        pred_boxes = torch.from_numpy(out[1]).to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbLM81cplyhx"
   },
   "outputs": [],
   "source": [
    "target_sizes = [[max(np.array(image).shape[:2]), max(np.array(image).shape[:2])]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZheCgil_YLih"
   },
   "outputs": [],
   "source": [
    "results = processor.post_process_object_detection(owl_out_1,\n",
    "                                                  threshold=0.6,\n",
    "                                                  target_sizes=target_sizes)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FaBYg6kNfCUZ",
    "outputId": "13966554-ae98-4a9b-d5eb-a4eb2bd1b80b"
   },
   "outputs": [],
   "source": [
    "results[\"boxes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfWdC-1-m8qh"
   },
   "source": [
    "We will get the most confident output which can exclude certain pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "-H0DXv3HgwQB",
    "outputId": "d8ad4649-67d0-4173-c516-1b9ba0c45579"
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image)\n",
    "\n",
    "box = [round(i) for i in results[\"boxes\"][1].tolist()]\n",
    "rectangle = patches.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], linewidth=2, edgecolor='b', facecolor='none')\n",
    "ax.add_patch(rectangle)\n",
    "\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3j4uOrDonVss"
   },
   "source": [
    "We will check the latency of the quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubGF-lGsnZU7"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "runtimes = []\n",
    "\n",
    "for _ in range(30):\n",
    "    start = time.time()\n",
    "\n",
    "    _ = session.run(['logits', 'pred_boxes', 'text_embeds', 'image_embeds'], np_inputs)\n",
    "    end = time.time()\n",
    "    runtimes.append(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DeRjG9Bri9I"
   },
   "source": [
    "Compared to 5336 ms it's quite an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OcKQrOWmriHB",
    "outputId": "b27c0b4c-846a-43d6-bc76-f8e91f296685"
   },
   "outputs": [],
   "source": [
    "print(f\"{sum(runtimes)/len(runtimes)}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPsynyyzmzo2g5N3aa4YOcC",
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
